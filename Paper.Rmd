---
title: "Reddit /r/Depression Authors who Pen Suicidal Ideation Leave Reddit at Higher Rates: An Attempt to Estimate Suicide Rates in a Data Void"
author: "Lacey Etzkorn"
date: "October 24, 2016"
output: 
      html_document:
            toc: TRUE
---

```{r setup, include=FALSE}
require(ggplot2); require(dplyr); require(reshape2)
options(width = 90)
options(digits = 4)
load("Data/SI_Author_Sample/June_Authors_Combined.RData")
```

Collaborators: Linda Gai

Peer Editors: Guanghao Qi

Supplemental Materials: [Supplement](Supplement.html).

# Introduction

The problem of suicide plagues social media and online communities devoted to mental health. 
Sadly, people who intend to commit suicide will often post last notes on sites such as Facebook, Twitter, and Reddit.
Additionally, health professionals often worry that unmonitored online interactions between individuals with mental illness could lead to exacerbation of symptoms and suicide clusters. 

In response, social media has assumed a more active role in monitoring and protecting the mental health of its users. Facebook currently has a report form for users who see suicidal content in friend posts. Twitter took an even more active approach in 2014 when they partnered with the charity Samaritans to release an app that automatically detected tweets indicating emotional distress, and notified the user's contacts that the user might need help. However, the app was discontinued shortly due to privacy concerns.

Given the delicate relationship between social media and suicide, it is of utmost importance that community moderators study the impacts of their policies on their community's rate of suicide. However, community moderators do not have inferential tools to do so. This is due, in part, to the lack of data available to moderators. It is near impossible for moderators to observe actual outcomes of suicide, especially when users are anonymous.

We propose an alternative parameter, similar in idea to a suicide rate, that community moderators could estimate with available data: the expected number of Reddit authors who abandon their accounts (drop out) after expressing suicidal ideation (SI). More concisely, we can call this parameter the expected SI-attributable drop outs. When we use the term "excess", we refer to the number of dropouts above what we would expect from an equal-sized comparable sample of authors who did not express suicidal ideation. This parameter is only defined for a population of authors who write in a given online community during a given time interval. For example, in this paper we define our population as the set of Reddit authors who post in /r/Depression during the month of June 2016. We compare the drop outs from the authors who expressed suicidal ideation in June /r/Depression (SI-postive authors), to a random sample of authors who did not express suicidal ideation in June /r/Depression (SI-negative authors). 

# Methods

The objective of our project is to estimate our parameter from Reddit data. Specifically, we will estimate the expected number drop-outs among authors who were identified as SI-positive in posts from June on /r/Depression. 

The process of collecting the necessary data and estimating this parameter can be broken down into 4 modular steps. 
First we scrape all posts and comments from /r/Depression in June 2016. The month June was chosen to ensure that we have at least three months of follow-up time for each author. An earlier month wasn't chosen because Reddit censors author comment and post history to the most recent 1,000 items each. 
(See [Supplement/Data Collection](Supplement.html).) 
Second, we classify whether each post or comment express SI. We label authors as SI-positive if they had any posts or comments with SI in June 2016 /r/Depression. Otherwise they are SI-negative. (See [SI Classification](#si) for details.)
Third, we collect all available post and comment data for every SI-positive author and a similar-sized random sample of SI-negative authors from June /r/Depression. [Code Reference]
Fourth, we use Monte-Carlo sampling to get a posterior distribution for the expected SI-attributable drop outs. (See [Estimation of Drop Outs](#estimation-of-drop-outs).)

### SI Definition

The goal for our classification algorithm is to determine whether comments and posts contain expressions of suicidal ideation (SI). From these classifications, we produce a set of authors that express SI in June /r/Depression. 

The CDC defines suicidal ideation, or thoughts about suicide, as "thoughts of engaging in suicide-related behavior" and "Thinking about, considering, or planning suicide" [2]. For our analysis, we change this definition to the following:

We define suicidal ideation (SI) as thinking about, considering, or planning suicide; desire to die; desire to stop living; or engagement in highly dangerous activities or self harm with the implicit goal of ending life. It may include admissions of having suicidal ideation in the past or the potential for suicidal ideation in the future. Evidence of suicidal ideation does not follow from hating or frustration with one's own life, extreme anguish or psychological distress, or engagement in highly dangerous activities or self harm where there is no implicit goal of ending one's life. However, explicit statements about wanting to die or engage in suicide are taken literally.

### SI Classification Algorithm

A number of methods have been implemented to detect suicidal ideation from user-generated text on social media [1]. However, here we use a simpler algorithm and leave the implementation of these other classifiers as a future project.

We expected SI expression to be fairly rare among /r/Depression posts. Thus, in order to get a good estimate of the drop out risk attributable to SI, we wanted our SI-positive sample to have a minimal proportion of true SI-negative authors. However, we were not worried about true SI-positive authors tainting our SI-negative sample since we expected SI-positive author to be so rare, and hence not have much influence on our estimate of the SI-negative drop out risk. In other words, we wanted a highly specific algorithm at the sacrifice of sensitivity.

To achieve this goal, we implement an algorithm that searches post text for at least one phrase from a reference list of SI-expressive phrases. We generated our reference list by programatically expanding a sample of SI-expressive phrases which were hand-picked from 750 random June /r/Depression comments. The algorithm is detailed in [Supplement/SI Classification](Supplement.html).

### Estimation of Drop Outs

Our goal is to estimate the number of SI-attributable drop outs. We do this by specifying a parametric Bayesian model and taking Monte-Carlo samples from the posterior. Mathematically, we write SI-attributable drop outs as:
$$E(\sum_i Q_i | SI_i=1) - E(\sum_i Q_i | SI_i=0)$$

Let $i$ index the authors. Let $Q_i$ be the (unobserved) event that author $i$ has quit Reddit. Let $t_i$ be the time since observing the most recent post from author $i$. Let $y_i$ be the (unobserved) length of time from the most recent post to the next post in the future. If author $i$ has quit Reddit, then $y_i = \infty$. Let $n_i$ be the observed number of intervals between posts for author $i$. Let $\bar{\epsilon_i}$ be the observed average time between Reddit posts for author $i$. Let $SI_i$ be the indicator that author $i$ is SI-positive in June. Let $\rho$ be the prior probability (which we define) that any author drops out of Reddit.

First assume that the post histories of each author are a Poisson process, and so observed times between posts are exponentially distributed with parameter $\lambda_i$. For each $\lambda_i$, specify the prior distribution $\lambda_i \sim gamma(1,1)$. 
Hence the posterior for $\lambda_i$ is 
$\lambda_i \big |n_i,\bar{\epsilon_i} \sim gamma(n_i + 1, 1 + n_i\cdot\bar{\epsilon_i})$.

Given $\lambda_i$ and prior $\rho$, we are able to calculate the probability that the author has stopped using Reddit using Bayes Rule.
$$ P(Q_i|\lambda_i, y_i > t_i, t_i, \rho)
            =  \frac{P(y_i > t_i| Q_i, \lambda_i, t_i, \rho) \cdot \rho}
                        {P(y_i > t_i|\lambda_i, t_i, \rho)}
            =   \frac{\rho} {e^{-\lambda_i\cdot t_i} \cdot (1-\rho) + \rho} 
$$ 
Note that $P(y_i > t_i| Q_i, \lambda_i, t_i, \rho) = 1$ since $y_i = \infty$ for the event $Q_i$. Also note that marginal to $Q_i$, $y_i$ follows a mixture of infinity with probability $\rho$ and an exponential with parameter $\lambda_i$ and probability $\rho$.

Now, if we consider each $Q_i$ to be independent, and we are given $P(Q_i|\lambda_i, y_i > t_i, t_i, \rho)$ for each author, then 
$$E(\sum_i Q_i| SI_i = 1) - E(\sum_i Q_i| SI_i = 0) = 
      \sum_i (-1)^{SI_i +1} P(Q_i|\lambda_i, y_i > t_i, t_i, \rho)$$

Hence, using these formulas, we can generate Monte-Carlo samples from the posterior distribution for the expected number of drop-outs attributable to SI.

# Results

### June /r/Depression

For the month of June, /r/Depression contained 22,041 posts, representing contributions from 7,572 authors. We identified 547 SI-positive posts from 492 unique authors. In a simple random sample of 100 SI-positive posts, and 100 SI-positive posts, we estimated the positive predictive value to be 88% (80%, 94%) and the negative predictive value to be 96% (90%, 99%).

### User Follow-Up

We were able to scrape post histories for 488 of the 492 SI-positive authors from June as well as 474 randomly sampled SI-negative authors from June.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.width=13, fig.height=6}
user.data = 
      author.summary %>%
      group_by(author) %>%
      filter(newest == max(newest)) %>%
      ungroup %>%
      group_by(si.author)%>%
      transmute(author, rank_newest = rank(newest)) %>%
      ungroup %>%
      select(-si.author) %>%
      merge(user.data, by = "author", all.y=T)
user.data$point.label = 
      ifelse(user.data$class, "SI",
      ifelse(user.data$r.depression, "/r/Depression","other")) %>%
      ordered(labels = c("Other Subreddit", "/r/Depression", "SI Expressive"))
user.data$si.group = 
      ifelse(user.data$si.author, "SI-Positive in June /r/Depression",
             "SI-Negative in June /r/Depression")
### Make plot
ggplot(data = subset(user.data, date > jan1)) +
geom_point(aes(y=rank_newest,x=date, 
               color=point.label, alpha=point.label, 
               size=point.label)) + 
ggtitle("Distribution of Post Dates across Authors between Jan. and Oct.") +
ylab("Author Rank by Newest Post") + xlab("Date") +
geom_vline(aes(xintercept = as.numeric(july1)), alpha=0.5) + 
geom_vline(aes(xintercept = as.numeric(june1)), alpha=0.5) + 
scale_colour_manual(name = "Post\nCharacteristics",
                    values = c("grey","darkblue", "red")) + 
scale_size_manual(name = "Post\nCharacteristics",
                  values = c(0.1,0.1,0.3)) + 
scale_alpha_manual(name = "Post\nCharacteristics",
                   values = c(0.1,0.1,1)) + 
theme_bw(20) + 
theme(legend.key = element_blank(),
      legend.position=c(1,0),
      legend.justification = c(1,0),
      legend.background = element_rect(fill=alpha('grey', 0)))+
facet_wrap("si.group") +
guides(color = guide_legend(override.aes = list(size=5, alpha = 1))) 
      
```

**Figure 1: Post histories for SI-Positive authors and SI-Negative authors are different.** Author ids are sorted along the y axis by time of the most recent post. Dates of posts are sorted along the x axis. Each horizontal strip of points represents a user history. Posts are red if they contain SI expression, regardless of their subreddit. Posts without SI expression are blue if they are in the subreddit /r/Depression, or they are otherwise grey. Vertical grey lines outline the month of June, from which data was sampled.

Figure 1 reveals a number of characteristics about the authors in our study. First, we note that those authors who posted more recently tended to post more often, which is expected. Second we see that many authors made multiple SI-positive posts, even among the group that was SI-negative in June /r/Depression.

Figure 1 also illuminates a number of important differences between the SI-positive and SI-negative groups. The figure shows that nearly 170 of our 491 authors had their last post in June, which was the period that we used to sample from. In comparison, only about 100 of the SI-negative authors had their last post in June. Fourth, of these 170 authors, many did not post frequently before dropping-off Reddit.

### Drop-Out Rate Estimates

```{r, echo=FALSE, warning=FALSE, fig.height=4, fig.width=5}
load("Data/Posteriors/Attributable_10e-10.Rdata")
ci = quantile(post.excess, probs = c(0.025, 0.975))
```

The histogram below represents the posterior distribution for the rate of drop outs for authors whose June /r/Depression comments were SI-positive.
Our 95% credible interval is (`r ci`). Hence, there is a 95% probability that the expected SI-attributable drop outs for authors in June 2016 /r/Depression is between `r ci[1]` and `r ci[2]`.

```{r, echo=FALSE, warning=FALSE, message=F,fig.height=4, fig.width=5}
ggplot() +
geom_histogram(aes(x=post.excess),  
               color="black", fill="blue", alpha=0.7) +
ggtitle("Posterior Distribution for\nSI-Attributable Drop-Outs (Prior = 10e-4)")+
xlab("Expected Number of SI-Attributable Drop-Outs") + 
ylab("Count") + 
theme_bw(14)
```

**Figure 2: 10,000 Posterior Monte-Carlo Samples for the Expected Number of SI-Attributable Drop-Outs.**

### Sensitivity to Priors

We found that our conclusions were sensitive to our specified prior probability that any author dropped out. The plot below shows posterior credible intervals for the expected number of SI-attributable drop outs in June /r/Depression.

If we set our prior probability of dropping out equal to 0.5, we would conclude that among the 491 authors who wrote SI-postitive posts in June /r/Depression, there were only about 14 excess reddit drop outs. However, when we set our prior probability of dropping out below $10^{-5}$, our estimates for excess dropouts associated with SI are in the range of 45 to 55.

```{r, echo=FALSE, warning=FALSE, fig.height=4, fig.width=5}
load("Data/Posteriors/Attributable_All.Rdata")
load("Data/Posteriors/Priors_All.Rdata")
library(scales)
ggplot() +
geom_errorbar(aes(ymax = post_p[3,], ymin = post_p[1,], x=prior.p)) +
geom_point(aes(y= post_p[2,], x=prior.p)) + 
scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
              labels = trans_format("log10", math_format(10^.x))) + 
theme_bw(14) + 
xlab("Prior Probability of Quitting Reddit") + 
ylab("Expected SI-Attributable Drop-Outs") +
ggtitle("Posterior Credible Intervals for\nTotal SI-Attributable Drop-Outs") 
```

**Figure 3: Posterior credible intervals for SI-attributable drop outs across many priors.** Each point represents a posterior median for SI-attributable drop outs, while bars represent 2.5% and 97.5% quantiles. Prior probabilities of dropping out range from $\rho = 0.5$ to $\rho=10^{-15.5}$

Given the discrepancy in conclusions we would draw between a large prior and small prior, we reccomend that a small prior be used for the following reason: large prior probabilities of dropping out inflate author-specific posterior probabilities of dropping out with artificial evidence. If instead we use a small prior-probability of dropping out, our posterior estimate that a given author has dropped out relies more heavily on their data: whether the time since an author's most recent post is unusually high compared to their post history. Hence, the estimates of total number of drop outs among SI-postitive and SI-negative authors respectively is (1) more conservative and (2) requires more data-based evidence. Consequently, the difference in estimated drop out rates between SI-positive and SI-negative authors is driven more by the data and less by the prior. 

In our previous results, we present a credible interval derived from a prior of $10^{-5}$ for this reason. The interpretation of this prior is as follows: without seeing any data, we would expect about 0.01 of our approximate 1,000 authors to drop out of reddit. 

# Discussion

Online communities present a unique platform where people can address their own mental health concerns. However, it has become increasingly difficult to ensure that these communities promote mental wellness. Social media lacks the inferential tools to study the mental health of it's online commuities, especially when it comes to suicide prevention. 

Most social media platforms do not have data about user suicides, and therefore can not estimate community suicide rates using traditional inferential tools. In this paper, we proposed an alternative parameter, SI-attributable drop outs, that should theoretically be linked to a suicide rate. However, it is extraordinarily important that researchers do not yet interpret this parameter as a suicide rate. Right now, it is only an estimate of the number of users who abandon their account after expressing suicidal ideation. A great deal of validation work would need to first establish a link between suicide rates and SI-attributable drop outs. For example, it would necessary to implement this method in an online community where true suicides could actually be observed. 

We list a number of extensions and adjustments for this project in order of priority.

First, our method does not properly account for counfounding relationships between SI and droping out. For example, moderators ban authors for "trolling." If being banned for trolling is more prevalent among SI-negative (or SI-positive) authors, our estimate of the drop outs attributable to SI will be biased downward (or upward).

Second, we need to adjust our estimates to account for the error in our SI classification algorithm. (See [June Depression](#june-/r/depression).) Not knowing whether each author truly expressed SI in their text data should inflate the uncertainty of the estimate.

Third, we need to propose a framework for extending our estimation longitudionally. Community moderators will want to measure SI-attributable drop outs before and after interventions. However, it is not yet clear how we will measure SI-attributable drop outs over time, especially when so many authors repeatedly express SI. In this paper, we only estimate the SI-attributable drop outs for June 2016 authors with data collected at a single time point (October 10, 2016). 

Fourth, we need to better assess the fit of our model to the observed data. We use a parametric Bayesian approach to estimate each author's individual probability of having dropped out. In this framework, we assumed that times between an author's posts follow an exponential distribution. However, mixture of exponential distributions with adjustments for temporal trends may better fit the data. 

Fifth, we ignore the information the author's text may contain about dropping out or attempting suicide. For example, we do not differentiate between suicidal ideation and suicide plans. A body of psychological literature documents the importance of accounting for and quantifying suicidal intent and planning when examining suicide risk [4]. However, attempting to mine such psychological measures from a large amount of text data is far beyond the scope of this paper and easily beyond the frontiers of psychological science and machine learning.

# References

1. Choudhury, M. D., Kiciman, E., Dredze, M., Coppersmith, G., & Kumar, M. (2016). Discovering Shifts to Suicidal Ideation from Mental Health Content in Social Media. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems - CHI '16. doi:10.1145/2858036.2858207

2. Crosby, A. E., Ortega, L., & Melanson, C. (2011). Self-directed violence surveillance: Uniform definitions and recommended data elements (CDC).

3. Eggerston, L. (2015, August 11). Social media embraces suicide prevention. Retrieved October 28, 2016, from http://www.cmaj.ca/content/187/11/E333.short

4. Nock, M. K., Borges, G., Bromet, E. J., Cha, C. B., Kessler, R. C., & Lee, S. (2008). Suicide and Suicidal Behavior. Epidemiologic Reviews, 30(1), 133–154. http://doi.org/10.1093/epirev/mxn002


