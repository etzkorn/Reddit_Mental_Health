---
title: "Supplemental Details"
author: "Lacey Etzkorn"
date: "October 26, 2016"
output: 
      html_document:
            toc: TRUE
---

```{r setup, message=FALSE, echo=FALSE}
library(ggplot2)
library(dplyr)
library(chron)
library(lubridate)
options(width = 200)
```

# Data Collection

The goal of our analysis is to estimate the expected number of drop-outs attributable to SI among June /r/Depression authors. The process of collecting the necessary data and estimating this parameter can be broken down into 4 modular steps:

1. **Collect all posts and comments from /r/depression in June 2016. **
We use the package subredditarchive, and modify the existing code from the python script here: https://github.com/peoplma/subredditarchive. The modified code that we used to scrape the June data can be found in 
[Scripts/0_Scrape_June_Data.py](Scripts/0_Scrape_June_Data.py).
We parse the json files for each thread for the main post and comments. For each, grab information on author, title (if applicable), text, creation date, name/link id, etc. 
(See code: [Scripts/1_Read_Json_Files.R](Scripts/1_Read_Json_Files.R))

2. **Combine and Classify June.**
Combine post and comment data into one data frame where the level of each observation is either a post or comment. Classify whether each post or comment from contained evidence of SI. (See [SI Classification](#si-classification)). 
Label authors as SI-positive if they had any posts or comments with SI in June 2016 /r/depression. Otherwise they are SI-negative. List unique SI-positive authors.
(See code: [Scripts/2_Combine_Classify_June_Posts_Comments.R](Scripts/2_Combine_Classify_June_Posts_Comments.R))

```{r, message=FALSE}
load(file = "Data/June_Depression_Sample/June_Posts_Classified.RData")
user.data$author[user.data$author == "\\[deleted\\]"] = NA
user.data$text[grepl("\\[deleted\\]",user.data$text)] = NA
summarise(user.data,
          total_posts = n(),
          total_si = sum(si),
          total_authors_si = length(unique(author[si])),
          unique_authors = length(unique(author)),
          unique_threads = length(unique(thread_id)))
authors = with(user.data, unique(author[si])) %>% na.omit()
authors.noSI = with(user.data, unique(author[!si])) %>% na.omit() %>% sample(500)
length(authors); length(authors.noSI)
rm(authors, authors.noSI, user.data)
```

3. **Scrape Author Data.**
Collect all available post and comment data for every SI-positive authors. Collect all available post and comment data for a similar-sized simple random sample of SI-negative authors. We collected ours at 4pm on October 10, 2016. 
We user this code to scrape the author data: [Scripts/3.1_Scrape_User_Data.R](Scripts/3.1_Scrape_User_Data.R),
[Scripts/3.2_Scrape_NoSI_User_Data.R](Scripts/3.2_Scrape_NoSI_User_Data.R). 
It takes nearly 24 hours to run. 
We use this code to classify the comments as SI-expressive:
[Scripts/4.1_Classify_SI_Author_Data.R](Scripts/4.1_Classify_SI_Author_Data.R),
[Scripts/4.2_Classify_NoSI_Author_Data.R](Scripts/4.2_Classify_NoSI_Author_Data.R).
This takes ~30 minutes to run. 
We use this code to combine the data and format it for plots, as well as summarize it for the estimation: 
[Scripts/5_Combine_Authors.R](Scripts/5_Combine_Authors.R).

```{r}
load("Data/SI_Author_Sample/June_Authors_Combined.RData")
user.data %>%
      group_by(si.author) %>%
      summarise(
          n_author = length(unique(author)),
          n_posts = n(),
          posts_per_author = n_posts / n_author,
          n_posts_si = sum(class, na.rm=T))
```

4. **Drop-Out Estimation.** Estimate the expected number of drop-outs attributable to SI. We save the posterior samples to plot in the paper.
(See code: [Scripts/6_MC_Estimation.R](Scripts/6_MC_Estimation.R))

# SI Classification

A number of methods have been implemented to detect suicidal ideation from user-generated text on social media [1]. However, here we use a simpler algorithm and leave the implementation of these other classifiers as a future project.

We expected SI expression to be fairly rare among /r/Depression posts. Thus, in order to get a good estimate of the drop out risk attributable to SI, we wanted our SI-positive sample to have a minimal proportion of true SI-negative authors. However, we were not worried about true SI-positive authors tainting our SI-negative sample since we expected SI-positive author to be so rare, and hence not have much influence on our estimate of the SI-negative drop out risk. In other words, we wanted a highly specific algorithm at the sacrifice of sensitivity.

To achieve this goal, we implement an algorithm that searches post text for at least one phrase from a reference list of SI-expressive phrases. We generated our reference list by programatically expanding a sample of SI-expressive phrases which were hand-picked from 750 random June /r/Depression comments. The algorithm is detailed in [Supplement/SI Classification](Supplement.html).

Phrase Recipes: 

* [I am] [intending to] [suicide verb]

* [I am] [thinking about] [suicide noun]

* I [intend to] [suicide verb]

* I [suicide verb]

* [I am] [suicide adjective]

* Why not [suicide verb]

* I [think about] [suicide noun]

* me [suicide verb]

* me [suicide noun]

* I [oppose] [live noun]

* I [am opposed to] [living]

##### SI Classification: PPV & NPV

We use the script [Scripts/99_Assign_Test_Values.R](Scripts/99_Assign_Test_Values.R) to sample 200 posts and hand-classify them as having true SI or not. 

```{r ppv, message = FALSE, warning=FALSE}
load(file = "Data/Test/Test_Classified.Rdata")
colnames(test.sample)
with(test.sample, table(si, suicidal))
library(Hmisc)
binconf(c(88, 96), c(100, 100), alpha=0.05, method=c("exact"))
```

We can see that the positive predictive value is 88% and the negative predictive value is 96%.

